{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/900 | Train Acc: 94.19%\n",
      "Epoch: 100/900 | Train Acc: 98.55%\n",
      "Epoch: 200/900 | Train Acc: 99.25%\n",
      "Epoch: 300/900 | Train Acc: 97.43%\n",
      "Epoch: 400/900 | Train Acc: 99.27%\n",
      "Epoch: 500/900 | Train Acc: 99.84%\n",
      "Epoch: 600/900 | Train Acc: 99.46%\n",
      "Epoch: 700/900 | Train Acc: 99.70%\n",
      "Epoch: 800/900 | Train Acc: 99.32%\n",
      "Epoch: 899/900 | Train Acc: 99.49%\n",
      "Test accuracy: 96.67%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(33.0, 0.5, 'True')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MLP algorithm with one hidden layer of size 40 taking data from Iris dataset\n",
    "\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Loading data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Scaling data\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# Training MLP using backpropagation algorithm with layers 4,40,3\n",
    "class MLP(object):\n",
    "    # Initializing parameters\n",
    "    def __init__(self, n_hidden=40, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, shuffle=True, minibatches=1, random_state=None):\n",
    "        self.random_state = random_state\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.cost_ = []\n",
    "    \n",
    "    # Encoding labels into one-hot representation    \n",
    "    def _encode_labels(self, y, k):\n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot\n",
    "    \n",
    "    # Initializing weights\n",
    "    def _initialize_weights(self):\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        w1 = rgen.normal(loc=0.0, scale=0.1, size=(self.n_hidden, X_train_std.shape[1]+1))\n",
    "        w2 = rgen.normal(loc=0.0, scale=0.1, size=(3, self.n_hidden+1))\n",
    "        return w1, w2\n",
    "    \n",
    "    # Sigmoid function and its derivative\n",
    "    def _sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(z, -250, 250)))\n",
    "    \n",
    "    def _sigmoid_gradient(self, z):\n",
    "        sg = self._sigmoid(z)\n",
    "        return sg * (1 - sg)\n",
    "    \n",
    "    # Adding bias unit to the input layer and the hidden layer \n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1]+1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0]+1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        return X_new\n",
    "    \n",
    "    # Feedforward propagation \n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        return a1, z2, a2, z3, a3\n",
    "    \n",
    "    # L1 and L2 regularization\n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) + np.sum(w2[:, 1:] ** 2))\n",
    "    \n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() + np.abs(w2[:, 1:]).sum())\n",
    "    \n",
    "    # Cost function and gradient calculation \n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1 - y_enc) * np.log(1 - output)\n",
    "        cost = np.sum(term1 - term2) + self._L1_reg(self.l1, w1, w2) + self._L2_reg(self.l2, w1, w2)\n",
    "        return cost\n",
    "    \n",
    "    # Backpropagation algorithm\n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        # Backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "        # Regularization and weight updates\n",
    "        grad1[:, 1:] += (w1[:, 1:] * (self.l1 + self.l2))\n",
    "        grad1[:, 1:] += w1[:, 1:] * self.l2\n",
    "        grad2[:, 1:] += (w2[:, 1:] * (self.l1 + self.l2))\n",
    "        grad2[:, 1:] += w2[:, 1:] * self.l2\n",
    "        return grad1, grad2\n",
    "    \n",
    "    # Predicting class labels \n",
    "    def predict(self, X):\n",
    "        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    # Training the neural network \n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        self.cost_ = []\n",
    "        X_data, y_data = X_train, y_train\n",
    "        y_enc = self._encode_labels(y_train, 3)\n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "        for i in range(self.epochs):\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X_data[idx], self.w1, self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx], output=a3, w1=self.w1, w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2, a3=a3, z2=z2, y_enc=y_enc[:, idx], w1=self.w1, w2=self.w2)\n",
    "                # update weights\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "                # each 100 epochs calculate and print the loss function and accuracy\n",
    "            if i%100 == 0 or i == self.epochs-1:\n",
    "                z2 = self.w1.dot(a1.T)\n",
    "                a2 = self._sigmoid(z2)\n",
    "                a2 = self._add_bias_unit(a2, how='row')\n",
    "                z3 = self.w2.dot(a2)\n",
    "                a3 = self._sigmoid(z3)\n",
    "                loss = -np.sum(y_enc[:, idx] * np.log(a3))\n",
    "                print('Epoch: %d/%d | Train Acc: %.2f%%' % (i, self.epochs, 100 - loss))\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "        return self\n",
    "\n",
    "# Training the neural network\n",
    "nn = MLP(n_hidden=40, l2=0.01, epochs=900, eta=0.005, alpha=0.001, decrease_const=0.00001, shuffle=True, minibatches=20, random_state=1)\n",
    "nn.fit(X_train_std, y_train, X_test_std, y_test)\n",
    "\n",
    "y_test_pred = nn.predict(X_test_std)\n",
    "acc = (np.sum(y_test == y_test_pred, axis=0)).astype(float) / X_test.shape[0]\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))\n",
    "\n",
    "# Showing the matrix of confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
